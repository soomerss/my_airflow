import json
import logging
from datetime import datetime, timedelta

import psycopg2
import requests
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

dag = DAG(
    dag_id="weather_etl",
    schedule="0 9 * * *",
    start_date=datetime(2023, 6, 9),
    catchup=False,
    max_active_runs=1,
)

# 원주
lat = 37.34201
lon = 127.9197


def get_curr_redshift():
    hook = PostgresHook(postgres_conn_id="redshift_dev_db")
    return hook.get_conn().cursor()


def extract(**context):
    url = f"""https://api.openweathermap.org/data/2.5/onecall?lat={lat}&lon={lon}&appid={context['params']['api_key']}&units=metric&lang=kr"""
    res = requests.get(url)
    return res.text


def transform(**context):
    data = context["task_instance"].xcom_pull(
        key="return_value", task_ids="weather_extract"
    )
    logging.info(data)
    data = json.loads(data)
    records = []
    for i in data["daily"]:
        date = datetime.utcfromtimestamp(i["dt"]) + timedelta(hours=9)
        maximum = i["temp"]["max"]
        minimum = i["temp"]["min"]
        records.append([date.strftime("%Y-%m-%d"), maximum, minimum])
    return records


# records의 값이 7개 밖에 되지 않으니 하나씩 Insert Into 방식으로 밀어넣기
# 여기서는 backfill은 필요 없어보임
# a -> temp. insertinto rownumber inssert intocopy


def load(**context):
    curr = get_curr_redshift()
    schema = context["params"]["schema"]
    table = context["params"]["table"]
    records = context["task_instance"].xcom_pull(
        key="return_value", task_ids="weather_transform"
    )

    try:
        # 임시테이블 생성 및 기존의 테이블에서 정보 가져가기
        sql = f"""CREATE TEMP TABLE t AS (SELECT * FROM {schema}.{table});
        """
        curr.execute(sql)

        for i in records:
            sql = f"""INSERT INTO t VALUES('{i[0]}',{i[1]},{i[2]},'{datetime.now()}')"""
            curr.execute(sql)
            print(sql)

        sql = f"""DELETE FROM {schema}.{table};"""
        curr.execute(sql)
        sql = f"""INSERT INTO {schema}.{table} (SELECT weather_date, maximum, minimum, created_at FROM 
        (SELECT *, ROW_NUMBER() OVER(PARTITION BY weather_date ORDER BY created_at desc) seq from t)
        where seq = 1);
        """
        curr.execute(sql)
        curr.execute("COMMIT;")

    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
        curr.execute("ROLLBACK;")
        raise


extract = PythonOperator(
    task_id="weather_extract",
    params={"api_key": Variable.get("weather_api_key")},
    python_callable=extract,
    dag=dag,
)

transform = PythonOperator(
    task_id="weather_transform", python_callable=transform, dag=dag
)

load = PythonOperator(
    task_id="weather_load",
    python_callable=load,
    params={"schema": "poqw741", "table": "weather_api"},
    dag=dag,
)

extract >> transform >> load
